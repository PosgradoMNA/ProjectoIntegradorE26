{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from skimage.draw import ellipse\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning of the anotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def remove_annotations_without_ulcer(file_path, output_path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file of image annotations, removes entries without the \"ulcer\" annotation,\n",
    "    and saves the cleaned data to a new JSON file. Prints the initial and final number of entries.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input JSON file with image annotations.\n",
    "        output_path (str): Path to save the output JSON file with filtered annotations.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the output JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Initial number of entries\n",
    "    initial_count = len(data)\n",
    "    print(f\"Initial number of IDs: {initial_count}\")\n",
    "\n",
    "    # Filter out entries without ulcer annotation\n",
    "    filtered_data = [\n",
    "        entry\n",
    "        for entry in data\n",
    "        if any(\n",
    "            annotation[\"from_name\"] == \"ulcer\"\n",
    "            for annotation in entry.get(\"annotations\", [])[0].get(\"result\", [])\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Final number of entries\n",
    "    final_count = len(filtered_data)\n",
    "    print(f\"Final number of IDs with 'ulcer' annotation: {final_count}\")\n",
    "\n",
    "    # Save filtered data to a new file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(filtered_data, f, indent=4)\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorneaUlcerDataset(Dataset):\n",
    "    def __init__(self, json_file, image_dir, transform=None, target_size=(256, 256)):\n",
    "        with open(json_file, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        image_name = entry[\"file_upload\"].split(\"-\")[-1]\n",
    "        img_path = os.path.join(self.image_dir, image_name)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "        annotations = entry[\"annotations\"][0][\"result\"]\n",
    "\n",
    "        # Loop over results to find \"Córnea\" and \"Úlcera\"\n",
    "        cornea_ellipse = None\n",
    "        ulcer_polygon = None\n",
    "        for result in annotations:\n",
    "            if result[\"from_name\"] == \"cornea\":\n",
    "                cornea_ellipse = result[\"value\"]\n",
    "            elif result[\"from_name\"] == \"ulcer\":\n",
    "                ulcer_polygon = result[\"value\"][\"points\"]\n",
    "\n",
    "        # Crop Image based on ellipse\n",
    "        if cornea_ellipse:\n",
    "            x_center = int(cornea_ellipse[\"x\"] / 100 * image.shape[1])\n",
    "            y_center = int(cornea_ellipse[\"y\"] / 100 * image.shape[0])\n",
    "            radius_x = int(cornea_ellipse[\"radiusX\"] / 100 * image.shape[1])\n",
    "            radius_y = int(cornea_ellipse[\"radiusY\"] / 100 * image.shape[0])\n",
    "\n",
    "            mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "            rr, cc = ellipse(y_center, x_center, radius_y, radius_x)\n",
    "            mask[rr, cc] = 255\n",
    "            cropped_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "        else:\n",
    "            cropped_image = image\n",
    "\n",
    "        # Resize cropped image to target size\n",
    "        cropped_image = cv2.resize(\n",
    "            cropped_image, self.target_size, interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "\n",
    "        # Create binary mask from ulcer polygon\n",
    "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        if ulcer_polygon:\n",
    "            points = [\n",
    "                (int(x / 100 * image.shape[1]), int(y / 100 * image.shape[0]))\n",
    "                for x, y in ulcer_polygon\n",
    "            ]\n",
    "            points = np.array([points], dtype=np.int32)\n",
    "            cv2.fillPoly(mask, points, 255)\n",
    "\n",
    "        # Resize mask to target size\n",
    "        mask = cv2.resize(mask, self.target_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Convert cropped image and mask to PIL images for transformations\n",
    "        cropped_image = Image.fromarray(cropped_image)\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            cropped_image = self.transform(cropped_image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return cropped_image, mask, image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_path, img_dir, batch_size=8):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Resize((256, 256))]\n",
    "    )\n",
    "    dataset = CorneaUlcerDataset(json_path, img_dir, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, dataloader, num_epochs=10, lr=0.001):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, masks, _ in dataloader:\n",
    "            # Move data to device -> TESTED IN M3\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            images = images.float()\n",
    "            masks = masks.float()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, feature_base=64):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder1 = self.conv_block(in_channels, feature_base)\n",
    "        self.encoder2 = self.conv_block(feature_base, feature_base * 2)\n",
    "        self.encoder3 = self.conv_block(feature_base * 2, feature_base * 4)\n",
    "        self.encoder4 = self.conv_block(feature_base * 4, feature_base * 8)\n",
    "\n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Downsample by a factor of 2\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(feature_base * 8, feature_base * 16)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(\n",
    "            feature_base * 16, feature_base * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = self.conv_block(feature_base * 16, feature_base * 8)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            feature_base * 8, feature_base * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = self.conv_block(feature_base * 8, feature_base * 4)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            feature_base * 4, feature_base * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = self.conv_block(feature_base * 4, feature_base * 2)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            feature_base * 2, feature_base, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = self.conv_block(feature_base * 2, feature_base)\n",
    "\n",
    "        # Final Convolution\n",
    "        self.final_conv = nn.Conv2d(feature_base, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.encoder1(x)\n",
    "        x2 = self.pool(x1)\n",
    "        x3 = self.encoder2(x2)\n",
    "        x4 = self.pool(x3)\n",
    "        x5 = self.encoder3(x4)\n",
    "        x6 = self.pool(x5)\n",
    "        x7 = self.encoder4(x6)\n",
    "        x8 = self.pool(x7)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(x8)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d4 = self.upconv4(bottleneck)\n",
    "        d4 = torch.cat((d4, x7), dim=1)\n",
    "        d4 = self.decoder4(d4)\n",
    "\n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = torch.cat((d3, x5), dim=1)\n",
    "        d3 = self.decoder3(d3)\n",
    "\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat((d2, x3), dim=1)\n",
    "        d2 = self.decoder2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat((d1, x1), dim=1)\n",
    "        d1 = self.decoder1(d1)\n",
    "        # Sigmoid for binary segmentation\n",
    "        return torch.sigmoid(self.final_conv(d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(json_path, img_dir, batch_size=8, train_ratio=0.8):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Resize((256, 256))]\n",
    "    )\n",
    "\n",
    "    # Load the full dataset\n",
    "    dataset = CorneaUlcerDataset(json_path, img_dir, transform=transform)\n",
    "\n",
    "    # Split the dataset\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoaders for train and test sets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of IDs: 218\n",
      "Final number of IDs with 'ulcer' annotation: 170\n"
     ]
    }
   ],
   "source": [
    "cleaned_anotations = remove_annotations_without_ulcer(\n",
    "    file_path=json_path, output_path=\"../data/cleaned_anotations.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n",
      "Initial number of IDs: 218\n",
      "Final number of IDs with 'ulcer' annotation: 170\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    mps_device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "json_path = \"../data/image_anotations.json\"\n",
    "cleaned_anotations = remove_annotations_without_ulcer(\n",
    "    file_path=json_path, output_path=\"../data/cleaned_anotations.json\"\n",
    ")\n",
    "img_dir = \"../data/raw\"\n",
    "train_loader, test_loader = create_dataloaders(\n",
    "    cleaned_anotations, img_dir, batch_size=4\n",
    ")\n",
    "\n",
    "\n",
    "model = UNet().to(mps_device)\n",
    "train_model(\n",
    "    model, device=mps_device, dataloader=train_loader, num_epochs=200, lr=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(model, device, dataloader, num_samples=5):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for faster inference\n",
    "        for i, (images, masks, image_names) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # Generate predictions\n",
    "            predictions = model(images)\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "\n",
    "            images = images.cpu().numpy()\n",
    "            masks = masks.cpu().numpy()\n",
    "            predictions = predictions.cpu().numpy()\n",
    "\n",
    "            # Plot each sample in the batch\n",
    "            for j in range(min(num_samples, images.shape[0])):\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                fig.suptitle(f\"Image name {image_names[j]}\")\n",
    "\n",
    "                # Original image\n",
    "                original_image = images[j].transpose(1, 2, 0)\n",
    "                axes[0].imshow(original_image)\n",
    "                axes[0].set_title(\"Original Image\")\n",
    "                axes[0].axis(\"off\")\n",
    "\n",
    "                # Ground truth mask\n",
    "                gt_mask = masks[j][0]\n",
    "                axes[1].imshow(original_image)\n",
    "                axes[1].imshow(gt_mask, cmap=\"jet\", alpha=0.5)\n",
    "                axes[1].set_title(\"Ground Truth Mask Overlay\")\n",
    "                axes[1].axis(\"off\")\n",
    "\n",
    "                # Predicted mask\n",
    "                pred_mask = predictions[j][0]\n",
    "                axes[2].imshow(original_image)\n",
    "                axes[2].imshow(pred_mask, cmap=\"jet\", alpha=0.5)\n",
    "                axes[2].set_title(\"Predicted Mask Overlay\")\n",
    "                axes[2].axis(\"off\")\n",
    "\n",
    "                plt.title(\"\")\n",
    "                plt.show()\n",
    "\n",
    "            if i * dataloader.batch_size >= num_samples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(model, dataloader=test_loader, device=mps_device, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_score(pred, target, threshold=0.3):\n",
    "    \"\"\"Calculate Intersection over Union (IoU) score.\"\"\"\n",
    "    pred = (pred > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    return (intersection / union).item() if union != 0 else 0\n",
    "\n",
    "\n",
    "def dice_score(pred, target, threshold=0.3):\n",
    "    \"\"\"Calculate Dice coefficient.\"\"\"\n",
    "    pred = (pred > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    return (\n",
    "        (2 * intersection / (pred.sum() + target.sum())).item()\n",
    "        if (pred.sum() + target.sum()) != 0\n",
    "        else 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, device, dataloader):\n",
    "    \"\"\"Evaluate the model using IoU and Dice score.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    iou_total, dice_total, count = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks, _ in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                iou = iou_score(outputs[i], masks[i])\n",
    "                dice = dice_score(outputs[i], masks[i])\n",
    "                iou_total += iou\n",
    "                dice_total += dice\n",
    "                count += 1\n",
    "\n",
    "    avg_iou = iou_total / count\n",
    "    avg_dice = dice_total / count\n",
    "    return {\"IoU\": avg_iou, \"Dice\": avg_dice}\n",
    "\n",
    "\n",
    "train_metrics = evaluate_model(model, device=mps_device, dataloader=train_loader)\n",
    "test_metrics = evaluate_model(model, device=mps_device, dataloader=test_loader)\n",
    "\n",
    "print(\"Training Metrics:\", train_metrics)\n",
    "print(\"Testing Metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "\n",
    "Durante esta fase del proyecto, nos enfocamos en la creación de máscaras tanto para córneas como para úlceras con el fin de evaluar el desempeño del modelo en la segmentación con los datos disponibles. Un aspecto importante fue el procesamiento de las anotaciones en formato JSON, las cuales contenían imágenes tanto con como sin máscara. Esto nos llevó a considerar la posibilidad de desarrollar un flujo adicional de preprocesamiento para filtrar o procesar las imágenes adecuadamente según sus etiquetas de máscara.\n",
    "\n",
    "Los primeros resultados en el conjunto de datos de prueba mostraron un desempeño de segmentación prometedor, logrando detecciones satisfactorias incluso con solo 50 imágenes etiquetadas. Estos resultados iniciales superaron nuestras expectativas, demostrando el potencial del modelo para identificar úlceras en imágenes no etiquetadas.\n",
    "\n",
    "Los próximos pasos incluyen finalizar el proceso de creación de máscaras, así como limpiar y estructurar el JSON para obtener mediciones concluyentes. Además, continuaremos optimizando la base de datos y enfocándonos en mejorar las métricas de rendimiento del modelo. Esto nos permitirá evaluar cuantitativamente la precisión de nuestro enfoque y compararlo con los modelos del estudio original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
